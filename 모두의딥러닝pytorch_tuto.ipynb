{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "모두의딥러닝pytorch_tuto.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP5lvxaJzyEiwncWmxn8Viy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ssifood/PyTorchZeroToAll/blob/master/%EB%AA%A8%EB%91%90%EC%9D%98%EB%94%A5%EB%9F%AC%EB%8B%9Dpytorch_tuto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhSXdKEVGrdd"
      },
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "     device = torch.device('cuda')\n",
        "     x = torch.empty(5,3)\n",
        "     y = torch.ones_like(x, device=device)\n",
        "     x = x.to(device)\n",
        "     z = x + y\n",
        "     print(z)\n",
        "     print(z.to('cpu',torch.double))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vw8N6639H32Z"
      },
      "source": [
        "#데이터\n",
        "x_train = torch.FloatTensor([[1],[2],[3]])\n",
        "y_train = torch.FloatTensor([[1],[2],[3]])\n",
        "#모델 초기화\n",
        "W = torch.zeros(1)\n",
        "#learning rate 설정\n",
        "lr = 0.01\n",
        "\n",
        "nb_epochs = 10\n",
        "for epoch in range(nb_epochs + 1): \n",
        "\n",
        "    #H(x) 계산\n",
        "    hypothesis = x_train * W\n",
        "    \n",
        "    #cost gradient 계산\n",
        "    cost = torch.mean((hypothesis - x_train)**2)\n",
        "    gradient = 2 * torch.mean((W * x_train - y_train) * x_train)  #이거 mean이 맞는것 같은데 왜 sum으로 했지..\n",
        "\n",
        "    print('Epoch {:4d}/{} W: {:.3f}, Cost: {:6f}'.format(epoch, nb_epochs, W.item(), cost.item()))\n",
        "\n",
        "    # cost gradient로 H(x) 개선\n",
        "    W -= lr* gradient"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDRAFuq-Kchb"
      },
      "source": [
        "#sum\n",
        "poch    0/10 W: 0.000, Cost: 14.000000\n",
        "Epoch    1/10 W: 0.093, Cost: 11.508623\n",
        "Epoch    2/10 W: 0.178, Cost: 9.460600\n",
        "Epoch    3/10 W: 0.255, Cost: 7.777033\n",
        "Epoch    4/10 W: 0.324, Cost: 6.393067\n",
        "Epoch    5/10 W: 0.387, Cost: 5.255385\n",
        "Epoch    6/10 W: 0.444, Cost: 4.320160\n",
        "Epoch    7/10 W: 0.496, Cost: 3.551363\n",
        "Epoch    8/10 W: 0.543, Cost: 2.919379\n",
        "Epoch    9/10 W: 0.586, Cost: 2.399859\n",
        "Epoch   10/10 W: 0.625, Cost: 1.972791"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uy3RsH5sMhwu"
      },
      "source": [
        "import torch\n",
        "#데이터\n",
        "x_train = torch.FloatTensor([[1],[2],[3]])\n",
        "y_train = torch.FloatTensor([[1],[2],[3]])\n",
        "\n",
        "#모델 초기화\n",
        "W = torch.zeros(1, requires_grad=True)\n",
        "#learning rate 설정\n",
        "lr = 0.01\n",
        "\n",
        "#optimizer 설정\n",
        "optimizer = torch.optim.SGD([W], lr)\n",
        "\n",
        "nb_epochs = 10\n",
        "for epoch in range(nb_epochs + 1):\n",
        "\n",
        "    #H(x) 계산\n",
        "    hypothesis = x_train * W\n",
        "    \n",
        "    #cost gradient 계산\n",
        "    cost = torch.mean((hypothesis - x_train)**2)\n",
        "\n",
        "    print('Epoch {:4d}/{} W: {:.3f}, Cost: {:6f}'.format(epoch, nb_epochs, W.item(), cost.item()))\n",
        "\n",
        "    #cost로 H(x) 개선\n",
        "    optimizer.zero_grad()  # optmizer에 있는 모든 그래드 초기화\n",
        "    cost.backward()         # cost값 미분해서 각 그래디언트 값 태움(삽입?)\n",
        "    optimizer.step()        #  저장된 그래디언트 값으로 실행.\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eweztGQuNMCG"
      },
      "source": [
        "#멀티 리그레션\n",
        "\n",
        "x_train = torch.FloatTensor([[73, 80, 75],\n",
        "                      [93,88,93],\n",
        "                      [89,91,90],\n",
        "                      [96,98,100],\n",
        "                      [73,66,70]])\n",
        "y_train = torch.FloatTensor([[152],[185],[180],[196],[142]])\n",
        "\n",
        "#모델 초기화\n",
        "W = torch.zeros((3,1), requires_grad=True)\n",
        "b = torch.zeros(1 , requires_grad=True)\n",
        "\n",
        "#optimizer \n",
        "optimizer = torch.optim.SGD([W,b], lr=1e-5)\n",
        "\n",
        "nb_epocs = 10\n",
        "\n",
        "\n",
        "for epoch in range(nb_epocs + 1):\n",
        "    #hypothesis 계산\n",
        "    hypotehsis = x_train.matmul(W) + b #or. mm or @ \n",
        "\n",
        "    #cost 계산\n",
        "    cost = torch.mean((hypotehsis - y_train) ** 2)\n",
        "\n",
        "    #cost로 H(x) 개선\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print('Epoch {:4d}/{} hypotehsis: {}, Cost: {:.6f}'.format(epoch, nb_epochs,\n",
        "                                                               hypotehsis.squeeze().detach(), cost.item() \n",
        "    ))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hui2vq5gVHUt"
      },
      "source": [
        "#멀티 리그레션\n",
        "\n",
        "x_train = torch.FloatTensor([[73, 80, 75],\n",
        "                      [93,88,93],\n",
        "                      [89,91,90],\n",
        "                      [96,98,100],\n",
        "                      [73,66,70]])\n",
        "y_train = torch.FloatTensor([[152],[185],[180],[196],[142]])\n",
        "\n",
        "\n",
        "\n",
        "#모델 초기화\n",
        "import torch.nn as nn\n",
        "class MultivariateLinearRegressionModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(3,1)\n",
        "\n",
        "    def forward(self,x):  #Hypothesis는 forward에서 \n",
        "        return self.linear(x)\n",
        "\n",
        "\n",
        "model = MultivariateLinearRegressionModel()\n",
        "\n",
        "#optimizer 설정\n",
        "#optimizer = torch.optim.SGD([w,b], lr=1e-5)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5)  #댓글에 어떤분이 답을줌... model.parameter임...\n",
        "\n",
        "\n",
        "nb_epocs = 10\n",
        "\n",
        "for epoch in range(nb_epocs + 1):\n",
        "    #hypothesis 계산\n",
        "    #hypotehsis = x_train.matmul(W) + b #or. mm or @ \n",
        "    hypotehsis = model(x_train)\n",
        "\n",
        "    #cost 계산\n",
        "    import torch.nn.functional as F\n",
        "    #cost 계산\n",
        "    cost = F.mse_loss(hypotehsis, y_train)\n",
        "\n",
        "    #cost로 H(x) 개선\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print('Epoch {:4d}/{} hypotehsis: {}, Cost: {:.6f}'.format(epoch, nb_epochs,\n",
        "                                                               hypotehsis.squeeze().detach(), cost.item() \n",
        "    ))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXqxHIeBYjV5"
      },
      "source": [
        "#pytorch DataSet\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self):\n",
        "        self.x_data = [[73, 80, 75],\n",
        "                       [93,88,93],\n",
        "                       [89,91,90],\n",
        "                       [96,98,100],\n",
        "                       [73,66,70]]\n",
        "        self.y_data = [[152], [185],[180],[196],[142]]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.FloatTensor(self.x_data[idx])\n",
        "        y = torch.FloatTensor(self.y_data[idx])\n",
        "        return x, y\n",
        "\n",
        "dataset = CustomDataset()\n",
        "\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "dataloader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size = 2, #통상 2의 배수로 한다.\n",
        "    shuffle = True\n",
        ")\n",
        "\n",
        "\n",
        "#모델 초기화\n",
        "import torch.nn as nn\n",
        "class MultivariateLinearRegressionModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(3,1)\n",
        "\n",
        "    def forward(self,x):  #Hypothesis는 forward에서 \n",
        "        return self.linear(x)\n",
        "\n",
        "\n",
        "model = MultivariateLinearRegressionModel()\n",
        "\n",
        "#optimizer 설정\n",
        "#optimizer = torch.optim.SGD([w,b], lr=1e-5)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5)  #댓글에 어떤분이 답을줌... model.parameter임...\n",
        "\n",
        "\n",
        "nb_epochs = 10\n",
        "\n",
        "for epoch in range(nb_epocs + 1):\n",
        "    for batch_idx , samples in enumerate(dataloader):\n",
        "        x_train, y_train = samples\n",
        "        #hypothesis 계산\n",
        "        prediction = model(x_train)\n",
        "\n",
        "        #cost 계산\n",
        "        import torch.nn.functional as F\n",
        "        #cost 계산\n",
        "        cost = F.mse_loss(prediction, y_train)\n",
        "\n",
        "        #cost로 H(x) 개선\n",
        "        optimizer.zero_grad()\n",
        "        cost.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        print('Epoch {:4d}/{} Batch: {}/{}, Cost: {:.6f}'.format(epoch, nb_epochs,\n",
        "                                                                batch_idx+1, len(dataloader),cost.item() \n",
        "        ))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIl7QXCi3HxC"
      },
      "source": [
        "#분류 모델, logistic Regression\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "#For reproductibility\n",
        "torch.manual_seed(1)\n",
        "\n",
        "x_data = [[1,2],[2,3],[3,1],[4,3],[5,3],[6,2]]\n",
        "y_data = [[0],[0],[0],[1],[1],[1]]\n",
        "\n",
        "x_train = torch.FloatTensor(x_data)\n",
        "y_train = torch.FloatTensor(y_data)\n",
        "\n",
        "\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "\n",
        "print('e_1 equals: ', torch.exp(torch.FloatTensor([1])))\n",
        "\n",
        "W = torch.zeros((2,1), requires_grad=True)\n",
        "b = torch.zeros(1, requires_grad=True)\n",
        "\n",
        "\n",
        "#모델 선언\n",
        "class BinaryClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(2,1)  #이거 데이터 내용이 중간에 바뀐듯..  8,1 안됌..!!!\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.sigmoid(self.linear(x))  \n",
        "\n",
        "model = BinaryClassifier()\n",
        "\n",
        "\n",
        "\n",
        "#optimizer 설정\n",
        "#optimizer = optim.SGD([W,b], lr=1)\n",
        "optimizer = optim.SGD(model.parameters(), lr=1)\n",
        "\n",
        "nb_epochs = 100\n",
        "\n",
        "'''\n",
        "#hypothesis = 1 / (1+ torch.exp(-(x_train.matmul(W)+b )))\n",
        "#hypothesis = 1 / (1+ torch.exp(-(torch.matmul(x_train,W)+b )))\n",
        "hypothesis = torch.sigmoid(x_train.matmul(W)+b)\n",
        "\n",
        "print(hypothesis)\n",
        "print(hypothesis.shape)\n",
        "\n",
        "\n",
        "#losses = (y_train[0] * torch.log(hypothesis[0]+ (1-y_train[0])* torch.log(1-hypothesis[0]) ))\n",
        "losses = -(y_train * torch.log(hypothesis) + \n",
        "                              (1-y_train)* torch.log(1-hypothesis))\n",
        "print(losses)\n",
        "\n",
        "\n",
        "print(losses.mean())\n",
        "\n",
        "F.binary_cross_entropy(hypothesis, y_train)\n",
        "'''\n",
        "for epoch in range(nb_epochs + 1):\n",
        "    #hypothesis = torch.sigmoid(torch.matmul(x_train,W)+b)\n",
        "    hypothesis = model(x_train)\n",
        "\n",
        "    cost = F.binary_cross_entropy(hypothesis, y_train)\n",
        "\n",
        "    optimizer.zero_grad() #이거 안하면 기존 그래디언트에 더한데..\n",
        "    cost.backward()\n",
        "    optimizer.step\n",
        "\n",
        "    #100번 마다 로그 출력\n",
        "    if epoch % 10 == 0:\n",
        "        prediction = hypothesis > torch.FloatTensor([0.5])\n",
        "        correct_prediction = prediction.float() == y_train\n",
        "        accuracy = correct_prediction.sum() / len(correct_prediction)\n",
        "        '''\n",
        "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
        "            epoch, nb_epochs, cost.item()\n",
        "        ))\n",
        "        '''\n",
        "        print('Epoch {:4d}/{} Cost: {:.6f} Accuracy: {:2.2f}'.format(\n",
        "            epoch, nb_epochs, cost.item(), accuracy*100\n",
        "        ))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S18L5rn9DTai"
      },
      "source": [
        "#softmax classification\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "#For reproductibility\n",
        "torch.manual_seed(1)\n",
        "\n",
        "\n",
        "z = torch.rand(3,5, requires_grad=True)\n",
        "hypothesis = F.softmax(z, dim=1)\n",
        "print(hypothesis)\n",
        "\n",
        "y = torch.randint(5, (3,)).long()\n",
        "print(y)\n",
        "\n",
        "y_one_hot = torch.zeros_like(hypothesis)\n",
        "y_one_hot.scatter_(1, y.unsqueeze(1),1)  #in_place  @@@조심조심..  (3,)  -> (3,1)\n",
        "\n",
        "cost = (y_one_hot * -torch.log(hypothesis)).sum(dim=1).mean()  #dim =0 은 어디지??\n",
        "print(cost)\n",
        "\n",
        "\n",
        "#torch.log(F.softmax(z, dim=1)) \n",
        "F.log_softmax(z, dim=1)\n",
        "\n",
        "\n",
        "#Low level\n",
        "#(y_one_hot * -torch.log(F.softmax(z, dim=1))).sum(dim=1).mean()\n",
        "(y_one_hot * -F.log_softmax(z,dim=1)).sum(dim=1).mean()\n",
        "\n",
        "#High level\n",
        "F.nll_loss(F.log_softmax(z,dim=1), y)  #NLL=Negative Log Likelihood\n",
        "\n",
        "#HIgh High level 제일 축약..\n",
        "F.cross_entropy(z,y)\n",
        "\n",
        "\n",
        "x_train = [[1,2,1,1],\n",
        "           [2,1,3,2],\n",
        "           [3,1,3,2],\n",
        "           [4,1,5,5],\n",
        "           [1,7,5,5],\n",
        "           [1,2,5,6],\n",
        "           [1,6,6,6],\n",
        "           [1,7,7,7]\n",
        "]\n",
        "\n",
        "y_train = [2,2,2,1,1,1,0,0]\n",
        "x_train = torch.FloatTensor(x_train)\n",
        "y_train = torch.LongTensor(y_train)\n",
        "\n",
        "\n",
        "#모델\n",
        "class SoftmaxClassifierModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(4,3) #outputdl 3!\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)  \n",
        "\n",
        "model = SoftmaxClassifierModel()\n",
        "\n",
        "\n",
        "\n",
        "#모델 초기화\n",
        "W = torch.zeros((4,3), requires_grad=True)\n",
        "b = torch.zeros(1, requires_grad=True)\n",
        "\n",
        "#optimizer = optim.SGD([W,b], lr=0.1)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "nb_epochs = 1000\n",
        "for epoch in range(nb_epochs + 1):\n",
        "    '''\n",
        "    hypothesis = F.softmax(x_train.matmul(W)+b, dim=1) # or .mm or @\n",
        "    y_one_hot = torch.zeros_like(hypothesis)\n",
        "    y_one_hot.scatter_(1,y_train.unsqueeze(1), 1)\n",
        "    cost = (y_one_hot * -torch.log(F.softmax(hypothesis, dim=1))).sum(dim=1).mean()\n",
        "    '''\n",
        "    #z  = torch.matmul(x_train,W) + b\n",
        "    #cost = F.cross_entropy(z, y_train)\n",
        "\n",
        "\n",
        "    prediction = model(x_train)\n",
        "    cost = F.cross_entropy(prediction, y_train)\n",
        "    \n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
        "            epoch, nb_epochs, cost.item()\n",
        "        ))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvijQsqkIA1S"
      },
      "source": [
        "#labs 07\n",
        "\n",
        "#Maximum Likelihood Estimation(MLE)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "#For reproductibility\n",
        "torch.manual_seed(1)\n",
        "\n",
        "\n",
        "x_train = torch.FloatTensor([[1,2,1],\n",
        "           [1,3,2],\n",
        "           [1,3,4],\n",
        "           [1,5,5],\n",
        "           [1,7,5],\n",
        "           [1,2,5],\n",
        "           [1,6,6],\n",
        "           [1,7,7]\n",
        "])\n",
        "\n",
        "y_train = torch.LongTensor([2,2,2,1,1,1,0,0])\n",
        "x_test = torch.FloatTensor([[2,1,1],[3,1,2],[3,3,4]])\n",
        "y_test = torch.LongTensor([2,2,2])\n",
        "\n",
        "# x_train  (m,3), y_train = (m,) x_test = (m^,3) y_test = (m^,)  맞춰줘야함..\n",
        "\n",
        "\n",
        "class SoftmaxClassificationModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(3,3)\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "\n",
        "model = SoftmaxClassificationModel()\n",
        "\n",
        "#optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "def train(model, optimizer, x_train, y_train):\n",
        "    nb_epochs = 20\n",
        "    for epoch in range(nb_epochs):\n",
        "        prediction = model(x_train)\n",
        "\n",
        "        cost = F.cross_entropy(prediction, y_train)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        cost.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        #print(epoch, nb_epochs,cost.item())\n",
        "        print('Epoch : {:4d}/{} Cost: {:.6f}'.format(epoch, nb_epochs, cost.item()\n",
        "        ))\n",
        "\n",
        "def test (model, optimizer, x_test, y_test):\n",
        "    prediction = model(x_test)\n",
        "    #print(prediction)\n",
        "    #print(1,prediction.max(1))\n",
        "    #print(2,prediction.max(1)[0])\n",
        "    predicted_classes = prediction.max(1)[1] #1차원 max이며 인덱스값?? [0]은 value고 [1]은 idx를 준다... 실험해봄..\n",
        "    #print(predicted_classes)\n",
        "    correct_count = (predicted_classes == y_test).sum().item()\n",
        "    cost = F.cross_entropy(prediction, y_test)\n",
        "\n",
        "\n",
        "    print('Accuracty: {}% Cost: {:.6f}'.format(\n",
        "        correct_count / len(y_test) * 100, cost.item()\n",
        "    ))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train(model, optimizer, x_train, y_train)\n",
        "print('---')\n",
        "test(model, optimizer, x_train, y_train)\n",
        "\n",
        "\n",
        "\n",
        "print('Data PreProcessing')\n",
        "#Data PreProcessing\n",
        "x_train = torch.FloatTensor([[73,80,75],\n",
        "                             [93,88,93],\n",
        "                             [89,91,90],\n",
        "                             [96,98,100],\n",
        "                             [73,66,70],\n",
        "                             ])\n",
        "y_train = torch.FloatTensor([[152],[185],[180],[196],[142]])\n",
        "\n",
        "#standardization (정규분포화 )\n",
        "mu = x_train.mean(dim=0)\n",
        "print(mu)\n",
        "sigma = x_train.std(dim=0)\n",
        "norm_x_train = (x_train - mu)/ sigma\n",
        "print(norm_x_train)\n",
        "\n",
        "class MultivariateLinearRegressionModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear()\n",
        "    def forward(self,x):\n",
        "        return self.linear(x)\n",
        "\n",
        "model = MultivariateLinearRegressionModel()\n",
        "optimizer = optim.SGD(model.parameters(), lr = 1e-1)\n",
        "\n",
        "def train1(model, optimizer, x_train, y_train):\n",
        "    nb_epochs = 20\n",
        "    for epoch in range(nb_epochs):\n",
        "        prediction = model(x_train)\n",
        "\n",
        "        cost = F.mse_loss(prediction, y_train)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        cost.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        #print(epoch, nb_epochs,cost.item())\n",
        "        print('Epoch : {:4d}/{} Cost: {:.6f}'.format(epoch, nb_epochs, cost.item()\n",
        "        ))\n",
        "\n",
        "train1(model, optimizer, x_train, y_train)\n",
        "print('---')\n",
        "test1(model, optimizer, x_train, y_train)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ADp3pkIJHsI"
      },
      "source": [
        "!pip install torchvision"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhsWIsBsFKhZ"
      },
      "source": [
        "#mnist\n",
        "\n",
        "import torch\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "#C , H, W  torch에 순서  toTensor가 순서 바꿔줌.\n",
        "\n",
        "# H, W C  일반적 순석\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# for reproducibility\n",
        "random.seed(777)\n",
        "torch.manual_seed(777)\n",
        "if device == 'cuda':\n",
        "    torch.cuda.manual_seed_all(777)\n",
        "\n",
        "# parameters\n",
        "training_epochs = 15\n",
        "batch_size = 100\n",
        "\n",
        "# MNIST dataset\n",
        "mnist_train = dsets.MNIST(root='MNIST_data/',\n",
        "                          train=True,\n",
        "                          transform=transforms.ToTensor(),\n",
        "                          download=True)\n",
        "\n",
        "mnist_test = dsets.MNIST(root='MNIST_data/',\n",
        "                         train=False,\n",
        "                         transform=transforms.ToTensor(),\n",
        "                         download=True)\n",
        "\n",
        "\n",
        "# dataset loader\n",
        "data_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=True,\n",
        "                                          drop_last=True)\n",
        "\n",
        "\n",
        "linear = torch.nn.Linear(784,10, bias=True).to(device)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.SGD(linear.parameters(), lr=0.1)\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "    total_batch = len(data_loader)\n",
        "\n",
        "    for X, Y in data_loader:\n",
        "        # reshape input image into [batch_size by 784]\n",
        "        # label is not one-hot encoded\n",
        "        X = X.view(-1, 28 * 28).to(device)\n",
        "        Y = Y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        hypothesis = linear(X)\n",
        "        cost = criterion(hypothesis, Y)\n",
        "        cost.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        avg_cost += cost / total_batch\n",
        "\n",
        "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
        "\n",
        "print('Learning finished')\n",
        "\n",
        "# Test the model using test sets\n",
        "with torch.no_grad():\n",
        "    X_test = mnist_test.test_data.view(-1, 28 * 28).float().to(device)\n",
        "    Y_test = mnist_test.test_labels.to(device)\n",
        "\n",
        "    prediction = linear(X_test)\n",
        "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
        "    accuracy = correct_prediction.float().mean()\n",
        "    print('Accuracy:', accuracy.item())\n",
        "\n",
        "    # Get one and predict\n",
        "    r = random.randint(0, len(mnist_test) - 1)\n",
        "    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float().to(device)\n",
        "    Y_single_data = mnist_test.test_labels[r:r + 1].to(device)\n",
        "\n",
        "    print('Label: ', Y_single_data.item())\n",
        "    single_prediction = linear(X_single_data)\n",
        "    print('Prediction: ', torch.argmax(single_prediction, 1).item())\n",
        "\n",
        "    plt.imshow(mnist_test.test_data[r:r + 1].view(28, 28), cmap='Greys', interpolation='nearest')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rt09kavVNoV_"
      },
      "source": [
        " #Lab 8 XOR\n",
        "import torch\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# for reproducibility\n",
        "torch.manual_seed(777)\n",
        "if device == 'cuda':\n",
        "    torch.cuda.manual_seed_all(777)\n",
        "\n",
        "\n",
        "\n",
        "X = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]]).to(device)\n",
        "Y = torch.FloatTensor([[0], [1], [1], [0]]).to(device)\n",
        "\n",
        "# nn layers\n",
        "linear = torch.nn.Linear(2, 1, bias=True)  #1layer라 구림.\n",
        "sigmoid = torch.nn.Sigmoid()\n",
        "\n",
        "# model\n",
        "model = torch.nn.Sequential(linear, sigmoid).to(device)\n",
        "\n",
        "# define cost/loss & optimizer\n",
        "criterion = torch.nn.BCELoss().to(device) #binary Cross entrophy loss 사용.\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1)\n",
        "\n",
        "\n",
        "for step in range(10001):\n",
        "    optimizer.zero_grad()\n",
        "    hypothesis = model(X)\n",
        "\n",
        "    # cost/loss function\n",
        "    cost = criterion(hypothesis, Y)\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if step % 100 == 0:\n",
        "        print(step, cost.item())\n",
        "    \n",
        "\n",
        "# Accuracy computation\n",
        "# True if hypothesis>0.5 else False\n",
        "with torch.no_grad():\n",
        "    hypothesis = model(X)\n",
        "    predicted = (hypothesis > 0.5).float()\n",
        "    accuracy = (predicted == Y).float().mean()\n",
        "    print('\\nHypothesis: ', hypothesis.detach().cpu().numpy(), '\\nCorrect: ', predicted.detach().cpu().numpy(), '\\nAccuracy: ', accuracy.item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bvb4IEJCPsYW"
      },
      "source": [
        "# Lab 9 XOR\n",
        "import torch\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# for reproducibility\n",
        "torch.manual_seed(777)\n",
        "if device == 'cuda':\n",
        "    torch.cuda.manual_seed_all(777)\n",
        "\n",
        "X = torch.FloatTensor([[0, 0], [0, 1], [1, 0], [1, 1]]).to(device)\n",
        "Y = torch.FloatTensor([[0], [1], [1], [0]]).to(device)\n",
        "\n",
        "# nn layers\n",
        "linear1 = torch.nn.Linear(2, 2, bias=True)\n",
        "linear2 = torch.nn.Linear(2, 1, bias=True)\n",
        "\n",
        "sigmoid = torch.nn.Sigmoid()\n",
        "\n",
        "# model\n",
        "model = torch.nn.Sequential(linear1, sigmoid, linear2, sigmoid).to(device)\n",
        "\n",
        "# define cost/loss & optimizer\n",
        "criterion = torch.nn.BCELoss().to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1)  # modified learning rate from 0.1 to 1\n",
        "\n",
        "\n",
        "for step in range(10001):\n",
        "    optimizer.zero_grad()\n",
        "    hypothesis = model(X)\n",
        "\n",
        "    # cost/loss function\n",
        "    cost = criterion(hypothesis, Y)\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if step % 100 == 0:\n",
        "        print(step, cost.item())\n",
        "\n",
        "# Accuracy computation\n",
        "# True if hypothesis>0.5 else False\n",
        "with torch.no_grad():\n",
        "    hypothesis = model(X)\n",
        "    predicted = (hypothesis > 0.5).float()\n",
        "    accuracy = (predicted == Y).float().mean()\n",
        "    print('\\nHypothesis: ', hypothesis.detach().cpu().numpy(), '\\nCorrect: ', predicted.detach().cpu().numpy(), '\\nAccuracy: ', accuracy.item())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXa08s1KUcls"
      },
      "source": [
        "#lab09 강에서 dropout, batch_norm 대충 합침.. 안돌아갈 수도 있음...\n",
        "\n",
        "\n",
        "import torch\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "import random\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# for reproducibility\n",
        "random.seed(111)\n",
        "torch.manual_seed(777)\n",
        "if device == 'cuda':\n",
        "    torch.cuda.manual_seed_all(777)\n",
        "\n",
        "learning_rate = 0.001\n",
        "training_epochs = 15\n",
        "batch_size = 100\n",
        "drop_prob = 0.3\n",
        "\n",
        "# MNIST dataset\n",
        "mnist_train = dsets.MNIST(root='MNIST_data/',\n",
        "                          train=True,\n",
        "                          transform=transforms.ToTensor(),\n",
        "                          download=True)\n",
        "\n",
        "mnist_test = dsets.MNIST(root='MNIST_data/',\n",
        "                         train=False,\n",
        "                         transform=transforms.ToTensor(),\n",
        "                         download=True)\n",
        "\n",
        "\n",
        "# dataset loader\n",
        "data_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=True,\n",
        "                                          drop_last=True)\n",
        "\n",
        "# nn layers\n",
        "linear1 = torch.nn.Linear(784, 256, bias=True)\n",
        "linear2 = torch.nn.Linear(256, 256, bias=True)\n",
        "linear3 = torch.nn.Linear(256, 10, bias=True)\n",
        "relu = torch.nn.ReLU()\n",
        "\n",
        "\n",
        "# Initialization\n",
        "torch.nn.init.normal_(linear1.weight)\n",
        "torch.nn.init.normal_(linear2.weight)\n",
        "torch.nn.init.normal_(linear3.weight)\n",
        "bn1 = torch.nn.BatchNorm1d(32)\n",
        "bn2 = torch.nn.BatchNorm1d(32)\n",
        "dropout = torch.nn.Dropout(p=drop_prob)\n",
        "\n",
        "# model\n",
        "model = torch.nn.Sequential(linear1, relu, linear2, relu, linear3).to(device)\n",
        "\n",
        "'''\n",
        "# model (드랍아웃은 활성화함수 뒤에서)\n",
        "model = torch.nn.Sequential(linear1, relu, dropout,\n",
        "                            linear2, relu, dropout,\n",
        "                            linear3, relu, dropout,\n",
        "                            linear4, relu, dropout,\n",
        "                            linear5).to(device)\n",
        "#batch_norm 은 활성화함수 앞에서 사용됨.\n",
        "bn_model = torch.nn.Sequential(linear1, bn1, relu,\n",
        "                            linear2, bn2, relu,\n",
        "                            linear3).to(device)\n",
        "'''\n",
        "# define cost/loss & optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)    # Softmax is internally computed.\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "total_batch = len(data_loader)\n",
        "for epoch in range(training_epochs):\n",
        "    modle.train()  #드랍아웃, 배치노말리제이션을 설정등을 타기에 매우중요.\n",
        "    avg_cost = 0\n",
        "\n",
        "    for X, Y in data_loader:\n",
        "        # reshape input image into [batch_size by 784]\n",
        "        # label is not one-hot encoded\n",
        "        X = X.view(-1, 28 * 28).to(device)\n",
        "        Y = Y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        hypothesis = model(X)\n",
        "        cost = criterion(hypothesis, Y)\n",
        "        cost.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        avg_cost += cost / total_batch\n",
        "\n",
        "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
        "\n",
        "print('Learning finished')\n",
        "\n",
        "\n",
        "# Test model and check accuracy\n",
        "with torch.no_grad():\n",
        "    model.eval()    # set the model to evaluation mode (dropout=False)\n",
        "\n",
        "    # Test the model using test sets\n",
        "    X_test = mnist_test.test_data.view(-1, 28 * 28).float().to(device)\n",
        "    Y_test = mnist_test.test_labels.to(device)\n",
        "\n",
        "    prediction = model(X_test)\n",
        "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
        "    accuracy = correct_prediction.float().mean()\n",
        "    print('Accuracy:', accuracy.item())\n",
        "\n",
        "    # Get one and predict\n",
        "    r = random.randint(0, len(mnist_test) - 1)\n",
        "    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float().to(device)\n",
        "    Y_single_data = mnist_test.test_labels[r:r + 1].to(device)\n",
        "\n",
        "    print('Label: ', Y_single_data.item())\n",
        "    single_prediction = model(X_single_data)\n",
        "    print('Prediction: ', torch.argmax(single_prediction, 1).item())"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}