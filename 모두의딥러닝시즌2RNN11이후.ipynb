{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "모두의딥러닝시즌2RNN11이후.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNcj9rAmYjN4mM587CM4Cfy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ssifood/PyTorchZeroToAll/blob/master/%EB%AA%A8%EB%91%90%EC%9D%98%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%8B%9C%EC%A6%8C2RNN11%EC%9D%B4%ED%9B%84.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJpxIAFe4VeY"
      },
      "source": [
        "#lab11-1 RNN BASIC\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Random seed to make results deterministic and reproducible\n",
        "torch.manual_seed(0)\n",
        "\n",
        "\n",
        "# declare dimension\n",
        "input_size = 4\n",
        "hidden_size = 2\n",
        "\n",
        "# singleton example\n",
        "# shape : (1, 1, 4)\n",
        "# input_data_np = np.array([[[1, 0, 0, 0]]])\n",
        "\n",
        "# sequential example\n",
        "# shape : (3, 5, 4)  3개의 배치, 5개의 글자길이, 4개의 벡터차원문자\n",
        "h = [1, 0, 0, 0]\n",
        "e = [0, 1, 0, 0]\n",
        "l = [0, 0, 1, 0]\n",
        "o = [0, 0, 0, 1]\n",
        "input_data_np = np.array([[h, e, l, l, o], [e, o, l, l, l], [l, l, e, e, l]], dtype=np.float32)\n",
        "\n",
        "# transform as torch tensor\n",
        "input_data = torch.Tensor(input_data_np)\n",
        "\n",
        "# declare RNN\n",
        "rnn = torch.nn.RNN(input_size, hidden_size)\n",
        "\n",
        "# check output\n",
        "outputs, _status = rnn(input_data)\n",
        "print(outputs)\n",
        "print(outputs.size())\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqFe2KsOZhpC"
      },
      "source": [
        "#lab11-2 RNN hihello and charseq\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "# Random seed to make results deterministic and reproducible\n",
        "torch.manual_seed(0)\n",
        "\n",
        "\n",
        "sample = \" if you want you\"\n",
        "\n",
        "# make dictionary\n",
        "char_set = list(set(sample))\n",
        "char_dic = {c: i for i, c in enumerate(char_set)}\n",
        "print(char_dic)\n",
        "\n",
        "# hyper parameters\n",
        "dic_size = len(char_dic)\n",
        "hidden_size = len(char_dic)\n",
        "learning_rate = 0.1\n",
        "\n",
        "# data setting\n",
        "sample_idx = [char_dic[c] for c in sample]\n",
        "x_data = [sample_idx[:-1]]\n",
        "x_one_hot = [np.eye(dic_size)[x] for x in x_data]  #이거 괜찮네.. 원핫코딩.\n",
        "print(x_one_hot)\n",
        "y_data = [sample_idx[1:]]\n",
        "\n",
        "# transform as torch tensor variable\n",
        "X = torch.FloatTensor(x_one_hot)\n",
        "Y = torch.LongTensor(y_data)\n",
        "\n",
        "# declare RNN\n",
        "rnn = torch.nn.RNN(dic_size, hidden_size, batch_first=True) \n",
        "#batch_firt 트루라고 하면 배치가 가장 먼저옴  B,S,F\n",
        "\n",
        "\n",
        "# loss & optimizer setting\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(rnn.parameters(), learning_rate)\n",
        "\n",
        "# start training\n",
        "for i in range(50):\n",
        "    optimizer.zero_grad()\n",
        "    outputs, _status = rnn(X)\n",
        "    loss = criterion(outputs.view(-1, dic_size), Y.view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    result = outputs.data.numpy().argmax(axis=2)\n",
        "    result_str = ''.join([char_set[c] for c in np.squeeze(result)])\n",
        "    print(i, \"loss: \", loss.item(), \"prediction: \", result, \"true Y: \", y_data, \"prediction str: \", result_str)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "933tflnadgMA"
      },
      "source": [
        "#11-3 longseq\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "# Random seed to make results deterministic and reproducible\n",
        "torch.manual_seed(0)\n",
        "\n",
        "sentence = (\"if you want to build a ship, don't drum up people together to \"\n",
        "            \"collect wood and don't assign them tasks and work, but rather \"\n",
        "            \"teach them to long for the endless immensity of the sea.\")\n",
        "\n",
        "# make dictionary\n",
        "char_set = list(set(sentence))\n",
        "char_dic = {c: i for i, c in enumerate(char_set)}\n",
        "\n",
        "# hyper parameters\n",
        "dic_size = len(char_dic)\n",
        "hidden_size = len(char_dic)\n",
        "sequence_length = 10  # Any arbitrary number\n",
        "learning_rate = 0.1\n",
        "\n",
        "# data setting\n",
        "x_data = []\n",
        "y_data = []\n",
        "\n",
        "for i in range(0, len(sentence) - sequence_length):\n",
        "    x_str = sentence[i:i + sequence_length]\n",
        "    y_str = sentence[i + 1: i + sequence_length + 1]\n",
        "    print(i, x_str, '->', y_str)\n",
        "\n",
        "    x_data.append([char_dic[c] for c in x_str])  # x str to index\n",
        "    y_data.append([char_dic[c] for c in y_str])  # y str to index\n",
        "\n",
        "x_one_hot = [np.eye(dic_size)[x] for x in x_data]\n",
        "\n",
        "\n",
        "# transform as torch tensor variable\n",
        "X = torch.FloatTensor(x_one_hot)\n",
        "Y = torch.LongTensor(y_data)\n",
        "\n",
        "\n",
        "''' 중복인것 같아 주석함\n",
        "# declare RNN + FC\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, layers):\n",
        "        super(Net, self).__init__()\n",
        "        self.rnn = torch.nn.RNN(input_dim, hidden_dim, num_layers=layers, batch_first=True)\n",
        "        self.fc = torch.nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, _status = self.rnn(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "'''\n",
        "# declare RNN + FC\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, layers):\n",
        "        super(Net, self).__init__()\n",
        "        self.rnn = torch.nn.RNN(input_dim, hidden_dim, num_layers=layers, batch_first=True)\n",
        "        self.fc = torch.nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, _status = self.rnn(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "net = Net(dic_size, hidden_size, 2)\n",
        "\n",
        "# loss & optimizer setting\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), learning_rate)\n",
        "\n",
        "# start training\n",
        "for i in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = net(X)\n",
        "    loss = criterion(outputs.view(-1, dic_size), Y.view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    results = outputs.argmax(dim=2)\n",
        "    #print(results)\n",
        "    predict_str = \"\"\n",
        "    for j, result in enumerate(results):\n",
        "        # print(i, j, ''.join([char_set[t] for t in result]), loss.item())\n",
        "        if j == 0:\n",
        "            predict_str += ''.join([char_set[t] for t in result])\n",
        "        else:\n",
        "            predict_str += char_set[result[-1]]\n",
        "\n",
        "    print(predict_str)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZsqIbi_hRbM"
      },
      "source": [
        "#lab 11-4 RNN timesieries\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Random seed to make results deterministic and reproducible\n",
        "torch.manual_seed(0)\n",
        "\n",
        "\n",
        "# scaling function for input data\n",
        "def minmax_scaler(data):\n",
        "    numerator = data - np.min(data, 0)\n",
        "    denominator = np.max(data, 0) - np.min(data, 0)\n",
        "    return numerator / (denominator + 1e-7)\n",
        "\n",
        "\n",
        "# make dataset to train\n",
        "\n",
        "# scaling function for input data\n",
        "def minmax_scaler(data):\n",
        "    numerator = data - np.min(data, 0)\n",
        "    denominator = np.max(data, 0) - np.min(data, 0)\n",
        "    return numerator / (denominator + 1e-7)\n",
        "\n",
        "# make dataset to input\n",
        "def build_dataset(time_series, seq_length):\n",
        "    dataX = []\n",
        "    dataY = []\n",
        "    for i in range(0, len(time_series) - seq_length):\n",
        "        _x = time_series[i:i + seq_length, :]\n",
        "        _y = time_series[i + seq_length, [-1]]  # Next close price\n",
        "        print(_x, \"->\", _y)\n",
        "        dataX.append(_x)\n",
        "        dataY.append(_y)\n",
        "    return np.array(dataX), np.array(dataY)\n",
        "\n",
        "\n",
        "# hyper parameters\n",
        "seq_length = 7  #7일\n",
        "data_dim = 5   #데이터 인풋 종류 5개\n",
        "hidden_dim = 10  #앞에서 10개로 하자고 햇음..\n",
        "output_dim = 1  # 종가\n",
        "learning_rate = 0.01\n",
        "iterations = 500\n",
        "\n",
        "\n",
        "# load data\n",
        "xy = np.loadtxt(\"data-02-stock_daily.csv\", delimiter=\",\")\n",
        "xy = xy[::-1]  # reverse order\n",
        "\n",
        "# split train-test set\n",
        "train_size = int(len(xy) * 0.7)\n",
        "train_set = xy[0:train_size]\n",
        "test_set = xy[train_size - seq_length:]\n",
        "\n",
        "# scaling data\n",
        "train_set = minmax_scaler(train_set)  #스케일을 0~1로 바꿈. 최소값 최대값 기준.\n",
        "test_set = minmax_scaler(test_set)  #스케일을 0~1로 바꿈. 최소값 최대값 기준.\n",
        "\n",
        "# make train-test dataset to input\n",
        "trainX, trainY = build_dataset(train_set, seq_length)\n",
        "testX, testY = build_dataset(test_set, seq_length)\n",
        "\n",
        "# convert to tensor\n",
        "trainX_tensor = torch.FloatTensor(trainX)\n",
        "trainY_tensor = torch.FloatTensor(trainY)\n",
        "\n",
        "testX_tensor = torch.FloatTensor(testX)\n",
        "testY_tensor = torch.FloatTensor(testY)\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, layers):\n",
        "        super(Net, self).__init__()\n",
        "        self.rnn = torch.nn.LSTM(input_dim, hidden_dim, num_layers=layers, batch_first=True)\n",
        "        self.fc = torch.nn.Linear(hidden_dim, output_dim, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, _status = self.rnn(x)\n",
        "        x = self.fc(x[:, -1])\n",
        "        return x\n",
        "\n",
        "\n",
        "net = Net(data_dim, hidden_dim, output_dim, 1)\n",
        "\n",
        "# loss & optimizer setting\n",
        "criterion = torch.nn.MSELoss()  #아웃풋이 실수값이라 썻데..?\n",
        "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
        "\n",
        "# start training\n",
        "for i in range(iterations):\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs = net(trainX_tensor)\n",
        "    loss = criterion(outputs, trainY_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(i, loss.item())\n",
        "\n",
        "plt.plot(testY)\n",
        "plt.plot(net(testX_tensor).data.numpy())\n",
        "plt.legend(['original', 'prediction'])\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dTaKCHYkrHB"
      },
      "source": [
        "#lab11-5 seq2seq\n",
        "\n",
        "# main reference\n",
        "# https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
        "\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "\n",
        "\n",
        "torch.manual_seed(0)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "raw = [\"I feel hungry.\t나는 배가 고프다.\",\n",
        "       \"Pytorch is very easy.\t파이토치는 매우 쉽다.\",\n",
        "       \"Pytorch is a framework for deep learning.\t파이토치는 딥러닝을 위한 프레임워크이다.\",\n",
        "       \"Pytorch is very clear to use.\t파이토치는 사용하기 매우 직관적이다.\"]\n",
        "\n",
        "\n",
        "# fix token for \"start of sentence\" and \"end of sentence\"\n",
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "\n",
        "# class for vocabulary related information of data\n",
        "class Vocab:\n",
        "    def __init__(self):\n",
        "        self.vocab2index = {\"<SOS>\": SOS_token, \"<EOS>\": EOS_token}\n",
        "        self.index2vocab = {SOS_token: \"<SOS>\", EOS_token: \"<EOS>\"}\n",
        "        self.vocab_count = {}\n",
        "        self.n_vocab = len(self.vocab2index)\n",
        "\n",
        "    def add_vocab(self, sentence):\n",
        "        for word in sentence.split(\" \"):\n",
        "            if word not in self.vocab2index:\n",
        "                self.vocab2index[word] = self.n_vocab\n",
        "                self.vocab_count[word] = 1\n",
        "                self.index2vocab[self.n_vocab] = word\n",
        "                self.n_vocab += 1\n",
        "            else:\n",
        "                self.vocab_count[word] += 1\n",
        "\n",
        "# filter out the long sentence from source and target data\n",
        "def filter_pair(pair, source_max_length, target_max_length):\n",
        "    return len(pair[0].split(\" \")) < source_max_length and len(pair[1].split(\" \")) < target_max_length\n",
        "\n",
        "\n",
        "\n",
        "# read and preprocess the corpus data\n",
        "def preprocess(corpus, source_max_length, target_max_length):\n",
        "    print(\"reading corpus...\")\n",
        "    pairs = []\n",
        "    for line in corpus:\n",
        "        pairs.append([s for s in line.strip().lower().split(\"\\t\")])\n",
        "    print(\"Read {} sentence pairs\".format(len(pairs)))\n",
        "\n",
        "    pairs = [pair for pair in pairs if filter_pair(pair, source_max_length, target_max_length)]\n",
        "    print(\"Trimmed to {} sentence pairs\".format(len(pairs)))\n",
        "\n",
        "    source_vocab = Vocab()\n",
        "    target_vocab = Vocab()\n",
        "\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        source_vocab.add_vocab(pair[0])\n",
        "        target_vocab.add_vocab(pair[1])\n",
        "    print(\"source vocab size =\", source_vocab.n_vocab)\n",
        "    print(\"target vocab size =\", target_vocab.n_vocab)\n",
        "\n",
        "    return pairs, source_vocab, target_vocab\n",
        "\n",
        "\n",
        "\n",
        "# declare simple encoder\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size) #input_size는 코퍼스의 단어의 갯수\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        x = self.embedding(x).view(1, 1, -1)\n",
        "        x, hidden = self.gru(x, hidden)\n",
        "        return x, hidden\n",
        "\n",
        "\n",
        "\n",
        "# declare simple decoder\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)  \n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        x = self.embedding(x).view(1, 1, -1)\n",
        "        x, hidden = self.gru(x, hidden)\n",
        "        x = self.softmax(self.out(x[0]))  #디코더랑 다른점..\n",
        "        return x, hidden\n",
        "\n",
        "# convert sentence to the index tensor\n",
        "def tensorize(vocab, sentence):\n",
        "    indexes = [vocab.vocab2index[word] for word in sentence.split(\" \")]\n",
        "    indexes.append(vocab.vocab2index[\"<EOS>\"])\n",
        "    return torch.Tensor(indexes).long().to(device).view(-1, 1)\n",
        "\n",
        "# training seq2seq\n",
        "def train(pairs, source_vocab, target_vocab, encoder, decoder, n_iter, print_every=1000, learning_rate=0.01):\n",
        "    loss_total = 0\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "\n",
        "    training_batch = [random.choice(pairs) for _ in range(n_iter)]\n",
        "    training_source = [tensorize(source_vocab, pair[0]) for pair in training_batch]\n",
        "    training_target = [tensorize(target_vocab, pair[1]) for pair in training_batch]\n",
        "\n",
        "    criterion = nn.NLLLoss()  #Negaitve log likelihood  loss\n",
        "\n",
        "    for i in range(1, n_iter + 1):\n",
        "        source_tensor = training_source[i - 1]\n",
        "        target_tensor = training_target[i - 1]\n",
        "\n",
        "        encoder_hidden = torch.zeros([1, 1, encoder.hidden_size]).to(device)\n",
        "\n",
        "        encoder_optimizer.zero_grad()\n",
        "        decoder_optimizer.zero_grad()\n",
        "\n",
        "        source_length = source_tensor.size(0)\n",
        "        target_length = target_tensor.size(0)\n",
        "\n",
        "        loss = 0\n",
        "\n",
        "        for enc_input in range(source_length):\n",
        "            _, encoder_hidden = encoder(source_tensor[enc_input], encoder_hidden)\n",
        "\n",
        "        decoder_input = torch.Tensor([[SOS_token]]).long().to(device)\n",
        "        decoder_hidden = encoder_hidden # connect encoder output to decoder input\n",
        "\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # teacher forcing gru \n",
        "            #결과를 다음셀에 넣기보다 직접 값을 넣어줌. 단점:불안해짐. 장점:수렴이 빨라짐.\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        encoder_optimizer.step()\n",
        "        decoder_optimizer.step()\n",
        "\n",
        "        loss_iter = loss.item() / target_length\n",
        "        loss_total += loss_iter\n",
        "\n",
        "        if i % print_every == 0:\n",
        "            loss_avg = loss_total / print_every\n",
        "            loss_total = 0\n",
        "            print(\"[{} - {}%] loss = {:05.4f}\".format(i, i / n_iter * 100, loss_avg))\n",
        "\n",
        "\n",
        "\n",
        "# insert given sentence to check the training\n",
        "def evaluate(pairs, source_vocab, target_vocab, encoder, decoder, target_max_length):\n",
        "    for pair in pairs:\n",
        "        print(\">\", pair[0])\n",
        "        print(\"=\", pair[1])\n",
        "        source_tensor = tensorize(source_vocab, pair[0])\n",
        "        source_length = source_tensor.size()[0]\n",
        "        encoder_hidden = torch.zeros([1, 1, encoder.hidden_size]).to(device)\n",
        "\n",
        "        for ei in range(source_length):\n",
        "            _, encoder_hidden = encoder(source_tensor[ei], encoder_hidden)\n",
        "\n",
        "        decoder_input = torch.Tensor([[SOS_token]], device=device).long()\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoded_words = []\n",
        "\n",
        "        for di in range(target_max_length):\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "            _, top_index = decoder_output.data.topk(1)\n",
        "            if top_index.item() == EOS_token:\n",
        "                decoded_words.append(\"<EOS>\")\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(target_vocab.index2vocab[top_index.item()])\n",
        "\n",
        "            decoder_input = top_index.squeeze().detach()\n",
        "\n",
        "        predict_words = decoded_words\n",
        "        predict_sentence = \" \".join(predict_words)\n",
        "        print(\"<\", predict_sentence)\n",
        "        print(\"\")\n",
        "\n",
        "# declare max length for sentence\n",
        "SOURCE_MAX_LENGTH = 10\n",
        "TARGET_MAX_LENGTH = 12\n",
        "\n",
        "\n",
        "# preprocess the corpus\n",
        "load_pairs, load_source_vocab, load_target_vocab = preprocess(raw, SOURCE_MAX_LENGTH, TARGET_MAX_LENGTH)\n",
        "print(random.choice(load_pairs))\n",
        "\n",
        "# declare the encoder and the decoder\n",
        "enc_hidden_size = 16\n",
        "dec_hidden_size = enc_hidden_size\n",
        "enc = Encoder(load_source_vocab.n_vocab, enc_hidden_size).to(device)\n",
        "dec = Decoder(dec_hidden_size, load_target_vocab.n_vocab).to(device)\n",
        "\n",
        "\n",
        "# train seq2seq model\n",
        "train(load_pairs, load_source_vocab, load_target_vocab, enc, dec, 5000, print_every=1000)\n",
        "\n",
        "# check the model with given data\n",
        "evaluate(load_pairs, load_source_vocab, load_target_vocab, enc, dec, TARGET_MAX_LENGTH)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwYD9Sh6qGAJ"
      },
      "source": [
        "#11-6 RNN PackedSequence\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "#예제 데이터\n",
        "#실습을 위해 간단한 예제 데이터를 만들었습니다. 여기서 잘 기억하셔야할 점은 batch size가 5이고, sequence 중 가장 긴 길이는 13라는 것 입니다.\n",
        "\n",
        "# Random word from random word generator\n",
        "data = ['hello world',\n",
        "        'midnight',\n",
        "        'calculation',\n",
        "        'path',\n",
        "        'short circuit']\n",
        "\n",
        "# Make dictionary\n",
        "char_set = ['<pad>'] + list(set(char for seq in data for char in seq)) # Get all characters and include pad token\n",
        "char2idx = {char: idx for idx, char in enumerate(char_set)} # Constuct character to index dictionary\n",
        "print('char_set:', char_set)\n",
        "print('char_set length:', len(char_set))\n",
        "\n",
        "# Convert character to index and make list of tensors\n",
        "X = [torch.LongTensor([char2idx[char] for char in seq]) for seq in data]\n",
        "\n",
        "# Check converted result\n",
        "for sequence in X:\n",
        "    print(sequence)\n",
        "\n",
        "\n",
        "# Make length tensor (will be used later in 'pack_padded_sequence' function)\n",
        "lengths = [len(seq) for seq in X]\n",
        "print('lengths:', lengths)\n",
        "\n",
        "'''\n",
        "Sequence 데이터의 경우 어떻게 batch로 묶을까요?\n",
        "위와같이 Text 나 audio 처럼 sequence 형식인 데이터의 경우 길이가 각각 다 다르기 때문에\n",
        "\n",
        "하나의 batch로 만들어주기 위해서 일반적으로 제일 긴 sequence 길이에 맞춰 뒷부분에 padding을 추가해줍니다.\n",
        "\n",
        "이 방식이 일반적으로 많이 쓰이는 Padding 방식입니다.\n",
        "\n",
        "하지만 PyTorch에서는 PackedSequence라는 것을 쓰면 padding 없이도 정확히 필요한 부분까지만 병렬 계산을 할 수 있습니다.\n",
        "\n",
        "pad_sequence 함수를 이용하여 PaddedSequence (그냥 Tensor) 만들기\n",
        "사실, PaddedSequence는 sequence중에서 가장 긴 sequence와 길이를 맞추어주기 위해 padding을 추가한 일반적인 Tensor를 말합니다.\n",
        "\n",
        "(따로 PaddedSequence라는 class는 존재하지 않습니다.)\n",
        "\n",
        "이때, pad_sequence라는 PyTorch 기본 라이브러리 함수를 이용하면 쉽게 padding을 추가할 수 있습니다.\n",
        "\n",
        "여기서 주의하실 점은 input이 Tensor들의 list 로 주어져야합니다. (그냥 Tensor 가 아닌 Tensor들의 list 입니다.)\n",
        "\n",
        "list 안에 있는 각각의 Tensor들의 shape가 (?, a, b, ...) 라고 할때, (여기서 ?는 각각 다른 sequence length 입니다.)\n",
        "\n",
        "pad_sequence 함수를 쓰면 (T, batch_size, a, b, ...) shape를 가지는 Tensor가 리턴됩니다. (여기서 T는 batch안에서 가장 큰 sequence length 입니다.)\n",
        "\n",
        "만약, pad_sequence에 명시적으로 batch_first=True라는 파라미터를 지정해주면,\n",
        "\n",
        "(batch_size, T, a, b, ...) shape를 가지는 Tensor가 리턴됩니다.\n",
        "\n",
        "기본적으로 padding 값은 0으로 되어있지만, padding_value=42와 같이 파라미터를 지정해주면, padding하는 값도 정할 수 있습니다.\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "# Make a Tensor of shape (Batch x Maximum_Sequence_Length)\n",
        "padded_sequence = pad_sequence(X, batch_first=True) # X is now padded sequence\n",
        "print(padded_sequence)\n",
        "print(padded_sequence.shape)\n",
        "\n",
        "'''\n",
        "pack_sequence 함수를 이용하여 PackedSequence 만들기\n",
        "PackedSequence는 위와같이 padding token을 추가하여 sequence의 최대 길이에 맞는 Tensor를 만드는게 아닌,\n",
        "\n",
        "padding을 추가하지 않고 정확히 주어진 sequence 길이까지만 모델이 연산을 하게끔 만드는 PyTorch의 자료구조입니다.\n",
        "\n",
        "이 PackedSequence를 만들기 위해서는 한가지 조건이 필요합니다.\n",
        "\n",
        "주어지는 input (list of Tensor)는 길이에 따른 내림차순으로 정렬이 되어있어야 합니다.\n",
        "따라서 먼저 input을 길이에 따른 내림차순으로 정렬해봅시다.\n",
        "\n",
        "'''\n",
        "\n",
        "# Sort by descending lengths\n",
        "sorted_idx = sorted(range(len(lengths)), key=lengths.__getitem__, reverse=True)\n",
        "sorted_X = [X[idx] for idx in sorted_idx]\n",
        "\n",
        "# Check converted result\n",
        "for sequence in sorted_X:\n",
        "    print(sequence)\n",
        "\n",
        "packed_sequence = pack_sequence(sorted_X)\n",
        "print(packed_sequence)\n",
        "\n",
        "'''\n",
        "Embedding 적용해보기\n",
        "자 이제, PackedSequence와 padding이 된 Tensor인 PaddedSequence를 만들어보았으니, RNN에 input으로 넣어서 테스트해보려고 합니다.\n",
        "\n",
        "그 전에, 위에 예제들에서는 input이 character의 index들을 가지고 있는 데이터였지만, 보통은 주로 이를 embedding한 값을 RNN의 input으로 넣어줍니다.\n",
        "\n",
        "이 튜토리얼에서는 one-hot character embedding을 해보도록 하겠습니다.\n",
        "'''\n",
        "\n",
        "# one-hot embedding using PaddedSequence\n",
        "eye = torch.eye(len(char_set)) # Identity matrix of shape (len(char_set), len(char_set))\n",
        "embedded_tensor = eye[padded_sequence]\n",
        "print(embedded_tensor.shape) # shape: (Batch_size, max_sequence_length, number_of_input_tokens)\n",
        "\n",
        "# one-hot embedding using PackedSequence\n",
        "embedded_packed_seq = pack_sequence([eye[X[idx]] for idx in sorted_idx])\n",
        "print(embedded_packed_seq.data.shape)\n",
        "\n",
        "'''\n",
        "RNN 모델 만들기\n",
        "간단한 RNN 모델을 한번 만들어봅시다.\n",
        "'''\n",
        "\n",
        "# declare RNN\n",
        "rnn = torch.nn.RNN(input_size=len(char_set), hidden_size=30, batch_first=True)\n",
        "\n",
        "rnn_output, hidden = rnn(embedded_tensor)\n",
        "print(rnn_output.shape) # shape: (batch_size, max_seq_length, hidden_size)\n",
        "print(hidden.shape)     # shape: (num_layers * num_directions, batch_size, hidden_size)\n",
        "\n",
        "rnn_output, hidden = rnn(embedded_packed_seq)\n",
        "print(rnn_output.data.shape)\n",
        "print(hidden.data.shape)\n",
        "\n",
        "'''\n",
        "pad_packed_sequence\n",
        "위 함수는 PackedSequence를 PaddedSequence(Tensor)로 바꾸어주는 함수입니다.\n",
        "\n",
        "PackedSequence는 각 sequence에 대한 길이 정보도 가지고있기 때문에, 이 함수는 Tensor와 함께 길이에 대한 리스트를 튜플로 리턴해줍니다.\n",
        "\n",
        "리턴값: (Tensor, list_of_lengths)\n",
        "'''\n",
        "\n",
        "unpacked_sequence, seq_lengths = pad_packed_sequence(embedded_packed_seq, batch_first=True)\n",
        "print(unpacked_sequence.shape)\n",
        "print(seq_lengths)\n",
        "\n",
        "'''\n",
        "pack_padded_sequence\n",
        "반대로, Padding이 된 Tensor인 PaddedSequence를 PackedSequence로 바꾸어주는 함수도 있습니다.\n",
        "\n",
        "pack_padded_sequence 함수는 실제 sequence길이에 대한 정보를 모르기때문에, 파라미터로 꼭 제공해주어야합니다.\n",
        "\n",
        "여기서 주의하여야 할 점은, input인 PaddedSequence가 아까 언급드린 길이에 따른 내림차순으로 정렬되어야 한다는 조건이 성립되어야 PackedSequence로 올바르게 변환될 수 있습니다.\n",
        "\n",
        "아까 저희가 만든 padded_sequence 변수는 이 조건을 만족하지 않기 때문에 다시 새로 만들어보겠습니다.\n",
        "'''\n",
        "\n",
        "embedded_padded_sequence = eye[pad_sequence(sorted_X, batch_first=True)]\n",
        "print(embedded_padded_sequence.shape)\n",
        "\n",
        "sorted_lengths = sorted(lengths, reverse=True)\n",
        "new_packed_sequence = pack_padded_sequence(embedded_padded_sequence, sorted_lengths, batch_first=True)\n",
        "print(new_packed_sequence.data.shape)\n",
        "print(new_packed_sequence.batch_sizes)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoKIVslUqPqS"
      },
      "source": [
        "PackedSequence 와 PaddedSequence\n",
        "링크: PackedSequence에 대한 PyTorch 공식 문서\n",
        "\n",
        "이 튜토리얼에서는 RNN / LSTM 계열의 모델에서 sequence batch를 잘 활용할 수 있는 PackedSequence 와 PaddedSequence를 만드는 법을 배워보겠습니다.\n",
        "\n",
        "PyTorch 라이브러리 안에는 다음 4가지 함수들이 주어집니다.\n",
        "\n",
        "pad_sequence, pack_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "하지만 함수 이름만 봐서는 상당히 헷갈릴 수 있기 때문에 다음 그림을 참고하시면 이해하기 편하실 것 같습니다.\n",
        "\n",
        "drawing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMU--EHFqhjk"
      },
      "source": [
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import pprint\n",
        "data = ['hello world',\n",
        "        'midnight',\n",
        "        'calculation',\n",
        "        'path',\n",
        "        'short circuit']\n",
        "\n",
        "\n",
        "char_set = ['<pad>'] + list(set(char for w in data for char in w))\n",
        "char2idx = { c:i for i,c in enumerate(char_set)}\n",
        "#char_set\n",
        "#char2idx\n",
        "X = [ torch.LongTensor([ char2idx[char] for char in seq]) for seq in data]\n",
        "lengths = [len(seq) for seq in X]\n",
        "padded_seq = pad_sequence(X, batch_first=True)\n",
        "#pprint.pprint(char2idx)\n",
        "#pprint.pprint(X)\n",
        "#print(padded_seq)\n",
        "\n",
        "#sorted_idx = sorted(range(len(lengths), key=lengths.__getitem__, reverse=True))\n",
        "sorted_idx = sorted(range(len(lengths)), key=lengths.__getitem__, reverse=True)\n",
        "sorted_X = [ X[idx] for idx in sorted_idx]\n",
        "sorted_idx2 = sorted(range(len(lengths)), reverse=True)\n",
        "#pprint.pprint(lengths)\n",
        "#pprint.pprint(sorted_idx)\n",
        "#pprint.pprint(sorted_idx2)\n",
        "pprint.pprint(X)\n",
        "pprint.pprint(sorted_X)\n",
        "packed_sequence = pack_sequence(sorted_X)\n",
        "packed_sequence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qu3WyTD-ODc"
      },
      "source": [
        "eye = torch.eye(len(char_set))  #대각행렬인가?? 그걸로 원핫 벡터만드네.\n",
        "#pprint.pprint(char_set)\n",
        "embedded_tensor = eye[padded_seq]\n",
        "embedded_tensor.shape  #5,13,19  5개 시퀀스, 13개 맥스길이, 19개 char\n",
        "embedded_packed_seq = pack_sequence([eye[X[idx]] for idx in sorted_idx])\n",
        "\n",
        "rnn = torch.nn.RNN(input_size=len(char_set), hidden_size=30, batch_first=True)\n",
        "\n",
        "rnn_output, hidden = rnn(embedded_tensor)\n",
        "#print(rnn_output)\n",
        "#print(rnn_output.shape)  #5,13,30\n",
        "#결괄르 텍스트로 바꾸는거 해보쟈.\n",
        "\n",
        "'''\n",
        "result = rnn_output.data.numpy().argmax(axis=2)\n",
        "result_str = ''.join([char_set[c] for c in np.squeeze(result)])\n",
        "print(result_str)\n",
        "'''\n",
        "#for ss in  rnn_output:\n",
        "#    print(''.join([char_set[w] for w in ss]))\n",
        "\n",
        "unpacked_sequence, seq_lengths = pad_packed_sequence(embedded_packed_seq, batch_first=True)\n",
        "\n",
        "\n",
        "embedded_padded_sequence = eye[pad_sequence(sorted_X, batch_first=True)]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}